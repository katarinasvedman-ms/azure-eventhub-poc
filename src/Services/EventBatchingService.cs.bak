using LogsysNgPoC.Models;
using System.Collections.Concurrent;
using System.Diagnostics;

namespace LogsysNgPoC.Services;

/// <summary>
/// Batches log events for efficient publishing to Event Hub.
/// 
/// BATCHING RATIONALE:
/// - Reduces individual API calls from 20k/sec to ~200 batches/sec
/// - Improves throughput by 10-100x
/// - Reduces network overhead and serialization cost
/// - Critical for meeting response time <200ms SLA
/// </summary>
public interface IEventBatchingService
{
    Task EnqueueEventAsync(LogEvent logEvent);
    Task<int> GetPendingEventCountAsync();
    event EventHandler<BatchReadyEventArgs>? BatchReady;
}

public class BatchReadyEventArgs : EventArgs
{
    public IReadOnlyList<LogEvent> Events { get; set; } = new List<LogEvent>();
    public int BatchSize { get; set; }
}

public class EventBatchingService : IEventBatchingService, IDisposable
{
    private readonly ConcurrentQueue<LogEvent> _eventQueue;
    private readonly Configuration.ApiOptions _options;
    private readonly ILogger<EventBatchingService> _logger;
    private readonly Timer _batchTimer;
    private readonly object _batchLock = new();
    private int _batchCount = 0;

    public event EventHandler<BatchReadyEventArgs>? BatchReady;

    public EventBatchingService(
        Microsoft.Extensions.Options.IOptions<Configuration.ApiOptions> options,
        ILogger<EventBatchingService> logger)
    {
        _eventQueue = new ConcurrentQueue<LogEvent>();
        _options = options.Value;
        _logger = logger;

        // Start timer for periodic batch flushing
        _batchTimer = new Timer(FlushBatchIfReady, null,
            TimeSpan.FromMilliseconds(_options.BatchTimeoutMs),
            TimeSpan.FromMilliseconds(_options.BatchTimeoutMs));
    }

    /// <summary>
    /// Enqueues an event for batching.
    /// Returns immediately (non-blocking).
    /// </summary>
    public async Task EnqueueEventAsync(LogEvent logEvent)
    {
        _eventQueue.Enqueue(logEvent);

        // Check if batch is full
        if (_eventQueue.Count >= _options.BatchSize)
        {
            await FlushBatchAsync();
        }
    }

    public async Task<int> GetPendingEventCountAsync()
    {
        return await Task.FromResult(_eventQueue.Count);
    }

    /// <summary>
    /// Flushes pending events as a batch.
    /// </summary>
    private void FlushBatchIfReady(object? state)
    {
        if (_eventQueue.Count > 0)
        {
            _ = FlushBatchAsync();
        }
    }

    private async Task FlushBatchAsync()
    {
        lock (_batchLock)
        {
            if (_eventQueue.Count == 0)
                return;

            var batch = new List<LogEvent>(_options.BatchSize);
            var stopwatch = Stopwatch.StartNew();

            while (batch.Count < _options.BatchSize && _eventQueue.TryDequeue(out var evt))
            {
                batch.Add(evt);
            }

            stopwatch.Stop();

            if (batch.Count > 0)
            {
                Interlocked.Increment(ref _batchCount);
                _logger.LogDebug("Flushing batch {BatchNumber} with {EventCount} events (prepared in {Ms}ms)",
                    _batchCount, batch.Count, stopwatch.ElapsedMilliseconds);

                BatchReady?.Invoke(this, new BatchReadyEventArgs
                {
                    Events = batch.AsReadOnly(),
                    BatchSize = batch.Count
                });
            }
        }
    }

    public void Dispose()
    {
        _batchTimer?.Dispose();
    }
}
